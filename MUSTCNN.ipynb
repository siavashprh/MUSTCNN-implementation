{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrjVrp4BXdcj",
        "outputId": "2779eaab-b8f0-48cb-f039-9cae5f239458"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "protein_tar_path = '/content/drive/MyDrive/MUST_CNN/4Protein.tar.gz'\n",
        "cb513_tar_path = '/content/drive/MyDrive/MUST_CNN/cb513_culldb.tar.gz'\n",
        "\n",
        "with tarfile.open(protein_tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=\"/content/4Protein\")\n",
        "with tarfile.open(cb513_tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=\"/content/cb513\")\n",
        "\n",
        "protein_data_path = '/content/4Protein/4Protein/data/aa1.dat'\n",
        "protein_label_path = '/content/4Protein/4Protein/data/dssp.lab.tag.dat'"
      ],
      "metadata": {
        "id": "q8UPORKZXgi-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, aa_file, label_file, max_length=300, num_classes=21):\n",
        "        self.max_length = max_length\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Load amino acid sequences\n",
        "        with open(aa_file, 'r') as f:\n",
        "            self.sequences = [list(map(int, line.strip().split())) for line in f]\n",
        "\n",
        "        # Load secondary structure labels\n",
        "        with open(label_file, 'r') as f:\n",
        "            self.labels = [list(map(int, line.strip().split())) for line in f]\n",
        "\n",
        "        assert len(self.sequences) == len(self.labels), \"Mismatch between sequences and labels\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.long) - 1  # Convert to zero-based index\n",
        "        labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "        # One-hot encode and pad sequences\n",
        "        one_hot_sequence = F.one_hot(sequence, num_classes=self.num_classes).float()\n",
        "        padded_sequence = F.pad(one_hot_sequence, (0, 0, 0, self.max_length - one_hot_sequence.shape[0]))\n",
        "\n",
        "        # Pad labels to match max length, using -1 for padding\n",
        "        padded_labels = F.pad(labels, (0, self.max_length - labels.shape[0]), value=-1)\n",
        "\n",
        "        return padded_sequence, padded_labels\n",
        "\n",
        "# Instantiate and verify the dataset\n",
        "dataset = ProteinDataset(protein_data_path, protein_label_path)\n",
        "print(\"Dataset length:\", len(dataset))\n",
        "print(\"Sample sequence shape:\", dataset[0][0].shape)\n",
        "print(\"Sample label shape:\", dataset[0][1].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWaFckSLWY0J",
        "outputId": "e065b50e-901d-48dd-9768-f80c09c30f34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 11794\n",
            "Sample sequence shape: torch.Size([300, 21])\n",
            "Sample label shape: torch.Size([300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping for indices to amino acids\n",
        "index_to_amino_acid = [\n",
        "    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'\n",
        "]\n",
        "\n",
        "# Decode function with index range check\n",
        "def decode_to_amino_acids(indices):\n",
        "    return [index_to_amino_acid[idx] for idx in indices if 0 <= idx < len(index_to_amino_acid)]\n",
        "\n",
        "# Decode the sample protein sequence\n",
        "decoded_amino_acids = decode_to_amino_acids(decoded_sequence)\n",
        "print(\"Sample protein sequence (as amino acid symbols):\")\n",
        "print(\"\".join(decoded_amino_acids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Gr5RLfXmsc",
        "outputId": "dddc6c0f-3289-467f-a546-7045f7e149d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample protein sequence (as amino acid symbols):\n",
            "CDEFGFHIGGFFFKCLLHMNMLFFPQCLRELSSMTVDWLGLYGTHWNWRPLMSFPHHRNNCWLLPNNNREHPDRCILVMNRMGPLCEKEITQYEKSGCFHQHCVVHQGPMHGCHFFLFSDLFFSGWHHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MUSTCNN(nn.Module):\n",
        "    def __init__(self, input_channels=21, num_classes=3, max_length=300):\n",
        "        super(MUSTCNN, self).__init__()\n",
        "\n",
        "        # Shift-and-stitch convolution layers with different dilations\n",
        "        self.shift_convs = nn.ModuleList([\n",
        "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1, dilation=1),\n",
        "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=3, dilation=3)\n",
        "        ])\n",
        "\n",
        "        # Additional convolutional layers\n",
        "        self.conv2 = nn.Conv1d(64 * len(self.shift_convs), 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        # Pooling and output layer\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
        "        self.output = nn.Conv1d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transpose input to (batch, channels, sequence_length)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply shift-and-stitch convolutions and concatenate results\n",
        "        shift_outputs = [F.relu(conv(x)) for conv in self.shift_convs]\n",
        "        x = torch.cat(shift_outputs, dim=1)\n",
        "\n",
        "        # Additional convolutions and pooling\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Output layer to produce class predictions for each position\n",
        "        x = self.output(x)\n",
        "\n",
        "        # Transpose to (batch, sequence_length, num_classes) for compatibility with labels\n",
        "        return x.permute(0, 2, 1)\n",
        "\n",
        "# Instantiate and verify the model\n",
        "model = MUSTCNN(input_channels=21, num_classes=3)\n",
        "print(\"Model structure:\", model)\n",
        "print(\"Output shape for a sample input:\", model(torch.randn(1, 300, 21)).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcEaPfGxZRFM",
        "outputId": "d825075c-eaa3-4f9b-89c5-a5396debe85c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: MUSTCNN(\n",
            "  (shift_convs): ModuleList(\n",
            "    (0): Conv1d(21, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (1): Conv1d(21, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
            "    (2): Conv1d(21, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "  )\n",
            "  (conv2): Conv1d(192, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool): MaxPool1d(kernel_size=2, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "  (output): Conv1d(256, 3, kernel_size=(1,), stride=(1,))\n",
            ")\n",
            "Output shape for a sample input: torch.Size([1, 301, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Split dataset into training and validation\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MUSTCNN(input_channels=21, num_classes=3).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore padding\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.reshape(-1, outputs.shape[-1])  # Flatten for loss calculation\n",
        "            labels = labels.reshape(-1)  # Flatten labels\n",
        "\n",
        "            # Compute loss, ignoring padding (-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track accuracy and loss\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            mask = labels != -1  # Ignore padding in accuracy calculation\n",
        "            correct += (predicted[mask] == labels[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print training metrics\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "# Run the updated training function\n",
        "train_model(model, train_loader, val_loader, num_epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Jj-KDB3nZme0",
        "outputId": "ea7c02fd-5b09-40c0-9d3b-83d02bcd98f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (9632) to match target batch_size (9600).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c27b6847a4f0>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Run the updated training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-c27b6847a4f0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Compute loss, ignoring padding (-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Backpropagation and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (9632) to match target batch_size (9600)."
          ]
        }
      ]
    }
  ]
}